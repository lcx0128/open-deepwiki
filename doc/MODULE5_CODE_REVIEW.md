# Module 5 ä»£ç å®¡æŸ¥æŠ¥å‘Š

> **ç‰ˆæœ¬**: 1.0.0 | **å®¡æŸ¥æ—¥æœŸ**: 2026-02-21
>
> **å®¡æŸ¥èŒƒå›´**: Module 5 å¤šè½®å¯¹è¯ RAG å±‚ï¼ˆå…¨é‡æ‰‹å·¥å®¡æŸ¥ + Codex è¾…åŠ©å®¡æŸ¥ï¼‰
>
> **å®¡æŸ¥æ–¹æ³•**: Gemini MCP å› ç½‘ç»œé”™è¯¯ï¼ˆECONNRESETï¼‰ä¸å¯ç”¨ï¼Œæ”¹ä¸ºæ‰‹å·¥é€æ–‡ä»¶å®¡æŸ¥ + OpenAI Codex è¾…åŠ©å®¡æŸ¥ã€‚

---

## å®¡æŸ¥æ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `app/schemas/mcp_types.py` | CodeGuideline / FileContext Pydantic æ¨¡å‹ |
| `app/schemas/chat.py` | ChatRequest / ChatResponse / ChatStreamEvent |
| `app/services/token_budget.py` | Token é¢„ç®—ä¼°ç®—ä¸è£å‰ª |
| `app/services/conversation_memory.py` | Redis ä¼šè¯ç®¡ç† |
| `app/services/query_fusion.py` | å¤šè½®æŸ¥è¯¢èåˆ |
| `app/services/two_stage_retriever.py` | åŒé˜¶æ®µ ChromaDB æ£€ç´¢ |
| `app/services/chat_service.py` | å¯¹è¯ä¸»æµç¨‹ç¼–æ’ |
| `app/api/chat.py` | FastAPI è·¯ç”±å±‚ |
| `app/services/embedder.py` | embed_query æ–°å¢å‡½æ•° |

---

## [CRITICAL] è¿è¡Œæ—¶é”™è¯¯

> ç»è¿‡å®Œæ•´å®¡æŸ¥ï¼Œ**æœªå‘ç°ä¼šå¯¼è‡´è¿è¡Œæ—¶å´©æºƒçš„ CRITICAL çº§åˆ«é—®é¢˜**ã€‚

ä»¥ä¸‹å…³é”®çº¦æŸå‡å·²æ­£ç¡®å®ç°ï¼š

- **ChromaDB çº¦æŸ** âœ… `stage1_discovery` ä½¿ç”¨ `query_embeddings=[query_vector]`ï¼ˆé¢„è®¡ç®—å‘é‡ï¼‰ï¼Œæœªä½¿ç”¨ `query_texts`ï¼Œç¬¦åˆ CLAUDE.md è§„å®šã€‚
- **Semaphore æ‡’åŠ è½½** âœ… `BaseLLMAdapter._semaphore` åœ¨ `_get_semaphore()` ä¸­æ‡’åŠ è½½ï¼›`embedder.py` çš„ `_embedding_semaphore` åŒæ ·æ‡’åŠ è½½ï¼Œæ— æ¨¡å—é¡¶å±‚åˆ›å»ºã€‚
- **DashScope LLM** âœ… æ‰€æœ‰ LLM è°ƒç”¨ä½¿ç”¨ `generate_with_rate_limit` / `stream_with_rate_limit`ï¼Œåº•å±‚è°ƒç”¨ `/chat/completions`ï¼Œæ—  `/responses` è°ƒç”¨ã€‚
- **å¯¼å…¥ä¸€è‡´æ€§** âœ… æ‰€æœ‰è¢« import çš„å‡½æ•°å‡å®é™…å­˜åœ¨ï¼š
  - `create_adapter(provider, model=None)` â€” `factory.py:12` âœ…
  - `get_redis()` â€” `redis_client.py:21`ï¼ˆasyncï¼‰, è°ƒç”¨å¤„å‡ä½¿ç”¨ `await` âœ…
  - `get_collection(repo_id)` â€” `embedder.py:56`ï¼ˆsyncï¼‰, è°ƒç”¨å¤„æ—  `await` âœ…
  - `embed_query(text)` â€” `embedder.py:197`ï¼ˆasyncï¼‰, è°ƒç”¨å¤„ä½¿ç”¨ `await` âœ…
  - `generate_with_rate_limit` â€” `adapter.py:46`ï¼Œè¿”å› `LLMResponse`ï¼Œæœ‰ `.content` å’Œ `.usage` âœ…
  - `stream_with_rate_limit` â€” `adapter.py:54`ï¼Œasync generatorï¼Œæ­£ç¡®ä½¿ç”¨ `async for` è¿­ä»£ âœ…
  - `LLMMessage(role=..., content=...)` â€” `schemas/llm.py:5`ï¼Œå­—æ®µä¸€è‡´ âœ…
  - `settings.DEFAULT_LLM_MODEL` â€” `config.py:20`ï¼Œå€¼ `"gpt-4o"` âœ…
  - `settings.REPOS_BASE_DIR` â€” `config.py:42`ï¼Œå€¼ `"./repos"` âœ…
- **async/await æ­£ç¡®æ€§** âœ… æ‰€æœ‰å¼‚æ­¥å‡½æ•°æ­£ç¡® awaitï¼Œæ‰€æœ‰åŒæ­¥å‡½æ•°æ— è¯¯ç”¨ awaitã€‚
- **å­—æ®µåŒ¹é…** âœ… `ChatResponse(**result)` ä¸­ `chunk_refs` ä¸º `List[dict]`ï¼ŒPydantic v2 è‡ªåŠ¨ä» dict æ„é€  `ChunkRef` å¯¹è±¡ã€‚
- **Redis decode_responses** âœ… `redis_client.py` ä½¿ç”¨ `decode_responses=True`ï¼Œ`hget` è¿”å› `str | None`ï¼Œ`json.loads()` æ­£ç¡®å¤„ç†ã€‚

---

## [WARNING] é€»è¾‘é—®é¢˜

### W1 â€” `apply_token_budget` çš„ break é€»è¾‘ä¼šè·³è¿‡å¯å®¹çº³çš„æ—§æ¶ˆæ¯

**æ–‡ä»¶**: `app/services/token_budget.py:68-76`

```python
for msg in reversed(messages):        # ä»æœ€æ–° â†’ æœ€æ—§éå†
    msg_tokens = estimate_tokens(msg.get("content", ""))
    if used_tokens + msg_tokens <= history_budget:
        trimmed_messages.insert(0, msg)
        used_tokens += msg_tokens
    else:
        break  # â† é—®é¢˜æ‰€åœ¨ï¼šä¸€æ—¦æŸæ¡æ¶ˆæ¯è¶…å‡ºé¢„ç®—ï¼Œå°±åœæ­¢æ·»åŠ æ›´æ—§çš„æ¶ˆæ¯
```

**åœºæ™¯**: è‹¥å†å²é¡ºåºä¸º `[A(100 tokens), B(8000 tokens), C(50 tokens)]`ï¼Œé¢„ç®— 200 tokensï¼š
- ä»æœ€æ–°å¼€å§‹è¿­ä»£ï¼šC(50) âœ“ â†’ B(8000) âœ— â†’ **breakï¼ˆA è¢«è·³è¿‡ï¼‰**
- ç»“æœï¼š`[C]`ï¼Œè€Œ A å…¶å®å¯ä»¥æ”¾å…¥ï¼ˆ50+100=150 â‰¤ 200ï¼‰

**å½±å“**: ä½æ¦‚ç‡ï¼ˆéœ€è¦å•æ¡è¶…å¤§æ¶ˆæ¯å¤¹åœ¨ä¸­é—´ï¼‰ï¼Œä½†ä¼šé™é»˜ä¸¢å¤±ä¸Šä¸‹æ–‡ã€‚

**å»ºè®®**: å°† `break` æ”¹ä¸º `continue`ï¼Œå…è®¸ç»§ç»­å°è¯•æ›´æ—§çš„æ¶ˆæ¯æ˜¯å¦èƒ½æ”¾å…¥é¢„ç®—ã€‚

---

### W2 â€” `conversation_memory.py` æ— æ¶ˆæ¯æ¡æ•°ä¸Šé™

**æ–‡ä»¶**: `app/services/conversation_memory.py:47-96`

`append_turn` æ¯æ¬¡è¿½åŠ  2 æ¡æ¶ˆæ¯ï¼ˆuser + assistantï¼‰ï¼Œ**æ— æ¡æ•°é™åˆ¶**ã€‚è§„æ ¼è¯´æ˜ï¼ˆDEVELOPMENT_SPEC.md / API.mdï¼‰è¦æ±‚ä¿ç•™æœ€è¿‘ 10 è½®å¯¹è¯ï¼ˆ20 æ¡æ¶ˆæ¯ï¼‰ï¼Œè¶…å‡ºæ—¶æ‰§è¡Œ FIFO æ·˜æ±°ã€‚

**å½±å“**: Redis Hash ä¸­ `messages` å­—æ®µä¼šéšå¯¹è¯è½®æ¬¡æ— é™å¢é•¿ï¼Œç›´è‡³ session TTLï¼ˆ24hï¼‰åˆ°æœŸã€‚`apply_token_budget` åœ¨ LLM è°ƒç”¨å‰ä¼šæŒ‰ Token é¢„ç®—è£å‰ªï¼ŒåŠŸèƒ½ä¸Šä¸ä¼šå´©æºƒï¼Œä½†ï¼š
1. Redis å­˜å‚¨ä¸å¿…è¦çš„å†å²æ•°æ®
2. `get_history` æ¯æ¬¡è¿”å›å…¨é‡æ¶ˆæ¯ï¼Œåºåˆ—åŒ–/ååºåˆ—åŒ–å¼€é”€å¢å¤§

**å»ºè®®**: åœ¨ `append_turn` ä¸­æ·»åŠ  `messages = messages[-20:]` ç¡®ä¿æœ€å¤šä¿ç•™ 20 æ¡ã€‚

---

### W3 â€” `handle_chat` / `handle_chat_stream` æ—  `repo_id` åˆæ³•æ€§æ ¡éªŒ

**æ–‡ä»¶**: `app/services/chat_service.py:47-157`

å½“ä¼ å…¥ä¸å­˜åœ¨çš„ `repo_id` æ—¶ï¼š
1. `stage1_discovery` è°ƒç”¨ `get_collection(repo_id)` â†’ ChromaDB çš„ `get_or_create_collection` ä¼š**é™é»˜åˆ›å»ºä¸€ä¸ªç©º collection**
2. æŸ¥è¯¢è¿”å›ç©ºç»“æœï¼ŒRAG ä¸Šä¸‹æ–‡ä¸ºç©º
3. LLM ä»ç„¶è°ƒç”¨å¹¶å›ç­”ï¼Œä½†å›ç­”æ— ä»£ç æ”¯æ’‘
4. ä¸ä¼šè§¦å‘ 404 é”™è¯¯ï¼Œç”¨æˆ·å¾—åˆ°çš„æ˜¯æ— æ„ä¹‰å›ç­”

**å»ºè®®**: åœ¨ session ç®¡ç†ä¹‹å‰ï¼ŒæŸ¥è¯¢ DB ç¡®è®¤ `repo_id` å­˜åœ¨ï¼š
```python
repo = await db.execute(select(Repository).where(Repository.id == repo_id))
if not repo.scalar_one_or_none():
    raise FileNotFoundError(f"ä»“åº“ä¸å­˜åœ¨: {repo_id}")
```

---

### W4 â€” `fuse_query` ä¸­ provider/model å¯èƒ½ä¸åŒ¹é…

**æ–‡ä»¶**: `app/services/query_fusion.py:53-55`

```python
adapter = create_adapter(llm_provider)          # å¦‚ dashscope
model = llm_model or "gpt-4o-mini"             # é»˜è®¤ gpt-4o-mini
response = await adapter.generate_with_rate_limit(..., model=model, ...)
```

è‹¥ `llm_provider="dashscope"` ä¸” `llm_model=None`ï¼Œåˆ™ DashScope é€‚é…å™¨ä¼šå°è¯•è°ƒç”¨ `gpt-4o-mini` æ¨¡å‹ï¼ˆè¯¥æ¨¡å‹åç§° DashScope ä¸è®¤è¯†ï¼‰ï¼Œå¯¼è‡´ API æŠ¥é”™ã€‚

**å·²æœ‰ç¼“è§£**: `except Exception as e: return question`ï¼Œé”™è¯¯è¢«é™é»˜åæ‰ï¼Œå›é€€åŸå§‹é—®é¢˜ã€‚åŠŸèƒ½ä¸ä¸­æ–­ï¼Œä½†æ¯æ¬¡è°ƒç”¨éƒ½ä¼šäº§ç”Ÿä¸€æ¬¡æ— æ•ˆ API è¯·æ±‚ã€‚

**å»ºè®®**: æ ¹æ® provider æä¾›åˆé€‚çš„é»˜è®¤è½»é‡æ¨¡å‹ï¼Œä¾‹å¦‚ï¼š
```python
DEFAULT_FUSION_MODELS = {
    "dashscope": "qwen-turbo",
    "gemini": "gemini-1.5-flash",
    "openai": "gpt-4o-mini",
    "custom": "gpt-4o-mini",
}
model = llm_model or DEFAULT_FUSION_MODELS.get(llm_provider or "openai", "gpt-4o-mini")
```

---

### W5 â€” æµå¼æ¨¡å¼ Token è®¡æ•°å§‹ç»ˆä¸º 0

**æ–‡ä»¶**: `app/services/chat_service.py:263`

```python
await append_turn(session_id, query, full_answer, chunk_refs, 0)  # 0 tokens
```

éæµå¼æ¨¡å¼é€šè¿‡ `response.usage.get("total_tokens", 0)` è·å¾—çœŸå® token ç”¨é‡ï¼Œæµå¼æ¨¡å¼æ— æ³•ä» AsyncIterator è·å– usage ä¿¡æ¯ï¼Œå¼ºåˆ¶å†™å…¥ 0ã€‚

**å½±å“**: Redis ä¼šè¯ä¸­ `total_tokens` å¯¹æµå¼å¯¹è¯æ°¸è¿œæ˜¯ 0ï¼Œæ— æ³•ç»Ÿè®¡çœŸå®ç”¨é‡ã€‚åŠŸèƒ½ä¸å½±å“ï¼Œä½†ç”¨é‡ç›‘æ§ä¸å‡†ç¡®ã€‚

**å»ºè®®**: åœ¨æµå¼ç”Ÿæˆå®Œæˆåï¼Œå¯¹ç”Ÿæˆå†…å®¹ä½¿ç”¨ `estimate_tokens(full_answer)` ä½œä¸ºè¿‘ä¼¼å€¼å†™å…¥ã€‚

---

## [INFO] æ”¹è¿›å»ºè®®

### I1 â€” `estimate_tokens` ç²¾åº¦è¾ƒä½

**æ–‡ä»¶**: `app/services/token_budget.py:5-9`

å½“å‰æ–¹æ¡ˆå¯¹ä»£ç ä¸­å¤§é‡ç‰¹æ®Šç¬¦å·ï¼ˆ`{}`, `[]`, `->`, `::` ç­‰ï¼‰çš„ token åŒ–ä¼°è®¡å¯èƒ½åå·® 2-3 å€ã€‚å¯¹äºä»£ç ç›¸å…³ RAGï¼Œå»ºè®®å¼•å…¥ `tiktoken` åº“è¿›è¡Œç²¾ç¡®è®¡ç®—ï¼ˆä»…éœ€ CPU æœ¬åœ°è®¡ç®—ï¼Œæ— ç½‘ç»œå¼€é”€ï¼‰ã€‚

---

### I2 â€” å†å²æ¶ˆæ¯ä¸­çš„é¢å¤–å­—æ®µæœªè¿‡æ»¤

**æ–‡ä»¶**: `app/services/chat_service.py:101-104`

Redis ä¸­å­˜å‚¨çš„å†å²æ¶ˆæ¯åŒ…å« `id`, `chunk_refs`, `timestamp` å­—æ®µï¼Œè€Œ `apply_token_budget` çš„ token ä¼°ç®—åªç”¨ `content` å­—æ®µã€‚è¿™æ˜¯æ­£ç¡®çš„ï¼Œä»£ç å·²è¿‡æ»¤ï¼š
```python
if role in ("user", "assistant"):
    messages.append(LLMMessage(role=role, content=msg.get("content", "")))
```
ä½† `apply_token_budget` çš„ `estimate_tokens(msg.get("content", ""))` ä¹Ÿåªå– `content`ï¼Œè¡Œä¸ºä¸€è‡´ã€‚âœ… æ— é—®é¢˜ï¼Œå¯ä»¥æ–‡æ¡£åŒ–è¯´æ˜ã€‚

---

### I3 â€” ChromaDB collection åç§°åœ¨ `stage1_discovery` ä¸­å¯èƒ½è¢«é‡å¤æ›¿æ¢

**æ–‡ä»¶**: `app/services/embedder.py:61` vs `app/services/two_stage_retriever.py:28`

`get_collection` å†…éƒ¨å°† `-` æ›¿æ¢ä¸º `_`ï¼Œå› æ­¤ `two_stage_retriever.py` ç›´æ¥è°ƒç”¨ `get_collection(repo_id)` æ˜¯æ­£ç¡®çš„ï¼Œä¸éœ€è¦é‡å¤å¤„ç†ã€‚âœ… æ— é—®é¢˜ã€‚

---

### I4 â€” `read_file_context` å‡½æ•°ç›®å‰æ— è°ƒç”¨æ–¹

**æ–‡ä»¶**: `app/services/two_stage_retriever.py:99-131`

`read_file_context` æ˜¯ä¸º Module 6 MCP å±‚é¢„ç•™çš„å·¥å…·å‡½æ•°ï¼Œå½“å‰ Module 5 æ²¡æœ‰è°ƒç”¨æ–¹ã€‚è¯¥å‡½æ•°å®šä¹‰æ­£ç¡®ï¼Œä¸å½±å“ç°æœ‰åŠŸèƒ½ï¼Œä½†å»ºè®®åœ¨ Module 6 å®ç°æ—¶å†æ­£å¼å¯ç”¨ã€‚

---

## æ€»ç»“

| çº§åˆ« | é—®é¢˜æ•° | çŠ¶æ€ |
|------|--------|------|
| **CRITICAL** | 0 | âœ… æ— è¿è¡Œæ—¶é”™è¯¯ |
| **WARNING** | 5 | âš ï¸ éœ€å…³æ³¨ï¼ˆä¸é˜»å¡ä¸Šçº¿ï¼‰ |
| **INFO** | 4 | ğŸ’¡ å»ºè®®ä¼˜åŒ– |

### æ•´ä½“è¯„ä¼°ï¼š**PASSï¼ˆå¯ä¸Šçº¿ï¼Œå»ºè®®åœ¨ä¸‹ä¸ªç‰ˆæœ¬ä¿®å¤ WARNING é—®é¢˜ï¼‰**

Module 5 å®ç°æ•´ä½“è´¨é‡è‰¯å¥½ï¼š
- æ‰€æœ‰å…³é”®çº¦æŸï¼ˆChromaDB `query_embeddings`ã€Semaphore æ‡’åŠ è½½ã€DashScope ä»… `/chat/completions`ï¼‰å‡å·²æ­£ç¡®éµå¾ª
- å¯¼å…¥é“¾å®Œæ•´ï¼Œæ— æœªå®šä¹‰è°ƒç”¨
- å¼‚æ­¥/åŒæ­¥ä½¿ç”¨æ­£ç¡®
- Pydantic å­—æ®µåŒ¹é…æ­£ç¡®ï¼ŒPydantic v2 è‡ªåŠ¨ç±»å‹å¼ºåˆ¶è½¬æ¢æ­£å¸¸å·¥ä½œ
- é”™è¯¯é™çº§è·¯å¾„ï¼ˆcontext_length exceededï¼‰æ­£ç¡®å®ç°
- Redis æ“ä½œï¼ˆhset mappingã€expireã€hgetï¼‰ä½¿ç”¨æ­£ç¡®

**ä¼˜å…ˆä¿®å¤**: W2ï¼ˆæ— æ¶ˆæ¯ä¸Šé™ï¼Œè¿åè§„æ ¼ï¼‰å’Œ W3ï¼ˆæ—  repo_id æ ¡éªŒï¼Œç”¨æˆ·ä½“éªŒé—®é¢˜ï¼‰ã€‚
W1ã€W4ã€W5 å¯åœ¨ä¸‹ä¸ªè¿­ä»£ä¿®å¤ã€‚

---

## æµ‹è¯•ç»“æœ

```
pytest tests/unit/test_token_budget.py tests/unit/test_query_fusion.py tests/unit/test_conversation_memory.py -v
============================= 50 passed in 1.02s ==============================

pytest tests/integration/test_chat_api.py -v
============================= 10 skipped in 0.03s =============================
(éœ€è¦è¿è¡Œä¸­çš„åç«¯æœåŠ¡æ‰èƒ½æ‰§è¡Œé›†æˆæµ‹è¯•)
```
